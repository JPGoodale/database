gptq:
  name: "gptq"

  image_url: https://www.marktechpost.com/wp-content/uploads/2023/08/Screenshot-2023-08-26-at-6.10.51-PM.png

  tags:
    - quantization

  url: https://github.com/IST-DASLab/gptq

  description: |
    GPTQ is a technique for post-training quantization, which is used to quantize large language models (LLMs) such as GPT.
    This method minimizes the storage requirements for GPT models by decreasing the bit count necessary to represent each
    weight within the model, reducing it from 32 bits to a mere 3-4 bits or even 2 bits.

  features:
    - "2, 3, 4 bit Weight Quantization for LLMs"
    - "Requires Calibration Dataset"
    - "Calibration free Zero Shot Quantization"
    - "Supports AMD GPU out of the box."
